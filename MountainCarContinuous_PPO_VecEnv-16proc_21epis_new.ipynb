{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCarContinuous-v0 with PPO, Vectorized Environment\n",
    "\n",
    "\n",
    "### 1. Create Vectorized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym version:  0.22.0\n",
      "torch version:  2.5.1\n",
      "device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_steps:  999\n",
      "threshold:  99.0\n",
      "reassigned threshold:  20000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from  collections  import deque\n",
    "import time\n",
    "from model import Policy\n",
    "from ppo import ppo_agent\n",
    "from storage import RolloutStorage\n",
    "from utils import get_render_func, get_vec_normalize\n",
    "from envs import make_vec_envs\n",
    "from parallelEnv import parallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('gym version: ', gym.__version__)\n",
    "print('torch version: ', torch.__version__)\n",
    "\n",
    "seed = 0 \n",
    "gamma=0.99\n",
    "num_processes =  16 \n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "envs = parallelEnv('MountainCarContinuous-v0', n=num_processes, seed=seed)\n",
    "\n",
    "## make_vec_envs -cannot find context for 'forkserver'\n",
    "## forkserver is only available in Python 3.4+ and only on some Unix platforms (not on Windows).\n",
    "## envs = make_vec_envs('BipedalWalker-v2', \\\n",
    "##                    seed + 1000, num_processes,\n",
    "##                    None, None, False, device='cpu', allow_early_resets=False)\n",
    "\n",
    "max_steps = envs.max_steps\n",
    "print('max_steps: ', max_steps)\n",
    "\n",
    "threshold = envs.threshold\n",
    "print('threshold: ', threshold)\n",
    "threshold = 20000000\n",
    "print('reassigned threshold: ', threshold)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "dir_chk = 'dir_save_test'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate Model, Agent and Storage\n",
    "\n",
    "Initialize the Policy (model MLPBase), PPO Agent and Rollout Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type obs:  <class 'numpy.ndarray'> , shape obs:  (16, 2)\n",
      "type obs_t:  <class 'torch.Tensor'> , shape obs_t:  torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "## model Policy uses MLPBase\n",
    "# policy = Policy(envs.observation_space.shape, envs.action_space,\\\n",
    "#         base_kwargs={'recurrent': False})\n",
    "\n",
    "# 修改policy\n",
    "policy = Policy(envs.observation_space.shape, envs.action_space,\\\n",
    "        base_kwargs={'recurrent': False})\n",
    "\n",
    "policy.to(device)\n",
    "\n",
    "agent = ppo_agent(actor_critic=policy, ppo_epoch=16, num_mini_batch=16,\\\n",
    "                 lr=0.01, eps=1e-5, max_grad_norm=0.5)\n",
    "\n",
    "rollouts = RolloutStorage(num_steps=max_steps, num_processes=num_processes, \\\n",
    "                        obs_shape=envs.observation_space.shape, action_space=envs.action_space, \\\n",
    "                        recurrent_hidden_state_size=policy.recurrent_hidden_state_size)\n",
    "\n",
    "obs = envs.reset()\n",
    "print('type obs: ', type(obs), ', shape obs: ', obs.shape)\n",
    "obs_t = torch.tensor(obs)\n",
    "print('type obs_t: ', type(obs_t), ', shape obs_t: ', obs_t.shape)\n",
    "\n",
    "rollouts.obs[0].copy_(obs_t)\n",
    "rollouts.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Save model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, directory, filename, suffix):\n",
    "    torch.save(model.base.actor.state_dict(), '%s/%s_actor_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.base.critic.state_dict(), '%s/%s_critic_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.base.critic_linear.state_dict(), '%s/%s_critic_linear_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.base, '%s/%s_model_base_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(model.dist, '%s/%s_model_dist_%s.pth' % (directory, filename, suffix))\n",
    "    \n",
    "limits = [-300, -160, -100, -70, -50, 0, 20, 30, 40, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330]\n",
    "\n",
    "def return_suffix(j):\n",
    "    suf = '0'\n",
    "    for i in range(len(limits)-1):\n",
    "        if j > limits[i] and j < limits[i+1]:\n",
    "            suf = str(limits[i+1])\n",
    "            break\n",
    "        \n",
    "        i_last = len(limits)-1    \n",
    "        if  j > limits[i_last]:\n",
    "            suf = str(limits[i_last])\n",
    "            break\n",
    "    return suf      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent  with Vectorized Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_updates=100\n",
    "gamma = 0.99\n",
    "tau=0.95\n",
    "save_interval=30\n",
    "log_interval= 1 \n",
    "\n",
    "def ppo_vec_env_train(envs, agent, policy, num_processes, num_steps, rollouts):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    n=len(envs.ps)    \n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    print('Number of agents: ', n)\n",
    "    envs.step([[1]*4]*n)\n",
    "    \n",
    "    indices = []\n",
    "    for i  in range(n):\n",
    "        indices.append(i)\n",
    "     \n",
    "    s = 0\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    avg_scores_array = []    \n",
    "\n",
    "    for i_episode in range(num_updates):\n",
    "        \n",
    "        total_reward = np.zeros(n)\n",
    "        timestep = 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        for timestep in range(num_steps):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                value, actions, action_log_prob, recurrent_hidden_states = \\\n",
    "                   policy.act(\n",
    "                        rollouts.obs[timestep],\n",
    "                        rollouts.recurrent_hidden_states[timestep],\n",
    "                        rollouts.masks[timestep])\n",
    "                   \n",
    "                \n",
    "            obs, rewards, done, _ = envs.step(actions.cpu().detach().numpy())\n",
    "            \n",
    "            \n",
    "            total_reward += rewards  ## this is the list by agents\n",
    "                        \n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            obs_t = torch.tensor(obs)\n",
    "            \n",
    "            ## Add one dimnesion to tensor, \n",
    "            ## This is (unsqueeze(1)) solution for:\n",
    "            ## RuntimeError: The expanded size of the tensor (1) must match the existing size...\n",
    "            rewards_t = torch.tensor(rewards).unsqueeze(1)\n",
    "            rollouts.insert(obs_t, recurrent_hidden_states, actions, action_log_prob, \\\n",
    "                value, rewards_t, masks)\n",
    "                                \n",
    "        avg_total_reward = np.mean(total_reward)\n",
    "        scores_deque.append(avg_total_reward)\n",
    "        scores_array.append(avg_total_reward)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            next_value = policy.get_value(rollouts.obs[-1],\n",
    "                            rollouts.recurrent_hidden_states[-1],\n",
    "                            rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, gamma, tau)\n",
    "        \n",
    "        agent.update(rollouts)\n",
    "        # 改动了一下update\n",
    "        # agent.update(rollouts, i_episode)\n",
    "\n",
    "        rollouts.after_update()\n",
    "        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "        avg_scores_array.append(avg_score)\n",
    "\n",
    "        if i_episode > 0 and i_episode % save_interval == 0:\n",
    "            print('Saving model, i_episode: ', i_episode, '\\n')\n",
    "            suf = return_suffix(avg_score)\n",
    "            save(policy, dir_chk, 'we0', suf)\n",
    "\n",
    "        \n",
    "        if i_episode % log_interval == 0 and len(scores_deque) > 1:            \n",
    "            prev_s = s\n",
    "            s = (int)(time.time() - time_start)\n",
    "            t_del = s - prev_s\n",
    "            print('Ep. {}, Timesteps {}, Score.Agents: {:.2f}, Avg.Score: {:.2f}, Time: {:02}:{:02}:{:02}, \\\n",
    "Interval: {:02}:{:02}'\\\n",
    "                   .format(i_episode, timestep+1, \\\n",
    "                        avg_total_reward, avg_score, s//3600, s%3600//60, s%60, t_del%3600//60, t_del%60)) \n",
    "    \n",
    "        if len(scores_deque) > 1 and avg_score > threshold:   \n",
    "            print('Environment solved with Average Score: ',  avg_score )\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return scores_array, avg_scores_array\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents:  16\n",
      "Ep. 1, Timesteps 999, Score.Agents: -69.01, Avg.Score: -68.08, Time: 00:00:02, Interval: 00:02\n",
      "Ep. 2, Timesteps 999, Score.Agents: -54.41, Avg.Score: -63.53, Time: 00:00:04, Interval: 00:02\n",
      "Ep. 3, Timesteps 999, Score.Agents: -46.63, Avg.Score: -59.30, Time: 00:00:05, Interval: 00:01\n",
      "Ep. 4, Timesteps 999, Score.Agents: -37.86, Avg.Score: -55.01, Time: 00:00:06, Interval: 00:01\n",
      "Ep. 5, Timesteps 999, Score.Agents: -32.11, Avg.Score: -51.20, Time: 00:00:07, Interval: 00:01\n",
      "Ep. 6, Timesteps 999, Score.Agents: -25.70, Avg.Score: -47.56, Time: 00:00:09, Interval: 00:02\n",
      "Ep. 7, Timesteps 999, Score.Agents: -22.24, Avg.Score: -44.39, Time: 00:00:10, Interval: 00:01\n",
      "Ep. 8, Timesteps 999, Score.Agents: -17.75, Avg.Score: -41.43, Time: 00:00:11, Interval: 00:01\n",
      "Ep. 9, Timesteps 999, Score.Agents: -14.10, Avg.Score: -38.70, Time: 00:00:12, Interval: 00:01\n",
      "Ep. 10, Timesteps 999, Score.Agents: -11.95, Avg.Score: -36.27, Time: 00:00:13, Interval: 00:01\n",
      "Ep. 11, Timesteps 999, Score.Agents: -9.59, Avg.Score: -34.04, Time: 00:00:15, Interval: 00:02\n",
      "Ep. 12, Timesteps 999, Score.Agents: -8.34, Avg.Score: -32.07, Time: 00:00:16, Interval: 00:01\n",
      "Ep. 13, Timesteps 999, Score.Agents: -7.02, Avg.Score: -30.28, Time: 00:00:17, Interval: 00:01\n",
      "Ep. 14, Timesteps 999, Score.Agents: -5.93, Avg.Score: -28.65, Time: 00:00:18, Interval: 00:01\n",
      "Ep. 15, Timesteps 999, Score.Agents: -5.00, Avg.Score: -27.18, Time: 00:00:20, Interval: 00:02\n",
      "Ep. 16, Timesteps 999, Score.Agents: -4.09, Avg.Score: -25.82, Time: 00:00:21, Interval: 00:01\n",
      "Ep. 17, Timesteps 999, Score.Agents: -3.35, Avg.Score: -24.57, Time: 00:00:22, Interval: 00:01\n",
      "Ep. 18, Timesteps 999, Score.Agents: -2.85, Avg.Score: -23.43, Time: 00:00:23, Interval: 00:01\n",
      "Ep. 19, Timesteps 999, Score.Agents: -2.36, Avg.Score: -22.37, Time: 00:00:25, Interval: 00:02\n",
      "Ep. 20, Timesteps 999, Score.Agents: -1.98, Avg.Score: -21.40, Time: 00:00:26, Interval: 00:01\n",
      "Ep. 21, Timesteps 999, Score.Agents: -1.64, Avg.Score: -20.50, Time: 00:00:27, Interval: 00:01\n",
      "Ep. 22, Timesteps 999, Score.Agents: -1.36, Avg.Score: -19.67, Time: 00:00:28, Interval: 00:01\n",
      "Ep. 23, Timesteps 999, Score.Agents: -1.12, Avg.Score: -18.90, Time: 00:00:30, Interval: 00:02\n",
      "Ep. 24, Timesteps 999, Score.Agents: -0.94, Avg.Score: -18.18, Time: 00:00:31, Interval: 00:01\n",
      "Ep. 25, Timesteps 999, Score.Agents: -0.80, Avg.Score: -17.51, Time: 00:00:32, Interval: 00:01\n",
      "Ep. 26, Timesteps 999, Score.Agents: -0.66, Avg.Score: -16.89, Time: 00:00:33, Interval: 00:01\n",
      "Ep. 27, Timesteps 999, Score.Agents: -0.58, Avg.Score: -16.31, Time: 00:00:34, Interval: 00:01\n",
      "Ep. 28, Timesteps 999, Score.Agents: -0.47, Avg.Score: -15.76, Time: 00:00:36, Interval: 00:02\n",
      "Ep. 29, Timesteps 999, Score.Agents: -0.40, Avg.Score: -15.25, Time: 00:00:37, Interval: 00:01\n",
      "Saving model, i_episode:  30 \n",
      "\n",
      "Ep. 30, Timesteps 999, Score.Agents: -0.33, Avg.Score: -14.77, Time: 00:00:38, Interval: 00:01\n",
      "Ep. 31, Timesteps 999, Score.Agents: -0.28, Avg.Score: -14.31, Time: 00:00:39, Interval: 00:01\n",
      "Ep. 32, Timesteps 999, Score.Agents: -0.23, Avg.Score: -13.89, Time: 00:00:41, Interval: 00:02\n",
      "Ep. 33, Timesteps 999, Score.Agents: -0.20, Avg.Score: -13.48, Time: 00:00:42, Interval: 00:01\n",
      "Ep. 34, Timesteps 999, Score.Agents: -0.17, Avg.Score: -13.10, Time: 00:00:43, Interval: 00:01\n",
      "Ep. 35, Timesteps 999, Score.Agents: -0.14, Avg.Score: -12.74, Time: 00:00:44, Interval: 00:01\n",
      "Ep. 36, Timesteps 999, Score.Agents: -788.85, Avg.Score: -33.72, Time: 00:00:46, Interval: 00:02\n",
      "Ep. 37, Timesteps 999, Score.Agents: -88.76, Avg.Score: -35.17, Time: 00:00:47, Interval: 00:01\n",
      "Ep. 38, Timesteps 999, Score.Agents: -60.32, Avg.Score: -35.81, Time: 00:00:48, Interval: 00:01\n",
      "Ep. 39, Timesteps 999, Score.Agents: -44.93, Avg.Score: -36.04, Time: 00:00:49, Interval: 00:01\n",
      "Ep. 40, Timesteps 999, Score.Agents: -2235.11, Avg.Score: -89.68, Time: 00:00:51, Interval: 00:02\n",
      "Ep. 41, Timesteps 999, Score.Agents: -2735.47, Avg.Score: -152.67, Time: 00:00:52, Interval: 00:01\n",
      "Ep. 42, Timesteps 999, Score.Agents: -5787.46, Avg.Score: -283.71, Time: 00:00:53, Interval: 00:01\n",
      "Ep. 43, Timesteps 999, Score.Agents: -5771.00, Avg.Score: -408.42, Time: 00:00:54, Interval: 00:01\n",
      "Ep. 44, Timesteps 999, Score.Agents: -5891.70, Avg.Score: -530.27, Time: 00:00:56, Interval: 00:02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores, avg_scores \u001b[38;5;241m=\u001b[39m ppo_vec_env_train(envs, agent, policy, num_processes, max_steps, rollouts)\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mppo_vec_env_train\u001b[0;34m(envs, agent, policy, num_processes, num_steps, rollouts)\u001b[0m\n\u001b[1;32m     66\u001b[0m     next_value \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mget_value(rollouts\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     67\u001b[0m                     rollouts\u001b[38;5;241m.\u001b[39mrecurrent_hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     68\u001b[0m                     rollouts\u001b[38;5;241m.\u001b[39mmasks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     70\u001b[0m rollouts\u001b[38;5;241m.\u001b[39mcompute_returns(next_value, gamma, tau)\n\u001b[0;32m---> 72\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(rollouts)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 改动了一下update\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# agent.update(rollouts, i_episode)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m rollouts\u001b[38;5;241m.\u001b[39mafter_update()\n",
      "File \u001b[0;32m~/Downloads/RL/Deep-Reinforcement-Learning-Algorithms/MountainCarContinuous_PPO/ppo.py:36\u001b[0m, in \u001b[0;36mppo_agent.update\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_epoch):\n\u001b[1;32m     34\u001b[0m     data_generator \u001b[38;5;241m=\u001b[39m rollouts\u001b[38;5;241m.\u001b[39mfeed_forward_generator(\n\u001b[1;32m     35\u001b[0m             advantages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_mini_batch)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data_generator:\n\u001b[1;32m     37\u001b[0m         obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n\u001b[1;32m     38\u001b[0m            return_batch, masks_batch, old_action_log_probs_batch, \\\n\u001b[1;32m     39\u001b[0m                 adv_targ \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# Reshape to do in a single forward pass for all steps\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, avg_scores = ppo_vec_env_train(envs, agent, policy, num_processes, max_steps, rollouts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model=policy,directory=dir_chk,filename='we0',suffix='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
